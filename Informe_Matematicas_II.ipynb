{
 "metadata": {
  "name": "",
  "signature": "sha256:956b69a0ced79bf71b72ebff0e8490ad800d2bef9ffbaab2d7fa68e3d0b14831"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Informe Final Matematicas II\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Presentado Por :"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alexadra Cepeda Vendina  \n",
      "Andres Guillermo Velasquez Gomez"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduccion \n",
      "A continuacion se explica la Aceleracion de la implementacion del metodo del metodo de Back Propagation, para el entrenamiento de redes neuronales, haciedo uso de la representacion en forma  matricial del metodo,lo cual nos permite generalizar la implementacion  para estructuras variables de redes neuronales. \n",
      "\n",
      "Para verificar la implemetacion del metodo, se entreno la red neuronal utilizando el dataset [MNIST](http://yann.lecun.com/exdb/mnist/ \"MNIST\") y para tener una implementacion de referencia se selecciono una de las arquitecturas planteadas en el paper disponible en la misma pagina donde se encuentra el dataset.\n",
      "\n",
      "#MNIST Dataset\n",
      "El dataset MNIST esta formado por digitos escritos a mano y se encuentras distribuidos en dos grupos, el primero esta confomado por 60.000 imagenes para el proceso de entrenamiento, y el segundo por 10.000 imagenes para realizar el proceso de pruebas.\n",
      "\n",
      "Las imagenes han sido centradas y todas tienen un tama\u00f1o fijo, estas imagenes fueron recolectadas de dos ploblaciones conformadas por  empleados del la oficina de censo de estados unidos y estudianes de colegios. cada dataset contiene igual cantidad de imagenes de las dos poblaciones.\n",
      "\n",
      "#Estructura de la Red Neuronal \n",
      "\n",
      "La arquitectura de la red neuronal con la que se realizo el proceso de pruebas de la implementacion del algoritmo esta comformada por 758 caracteristicas de los 28*28  pixeles de las imagenes mas el termino independiente conosido como bias el cual siempre es igual a 1 (capa de entrada).  en la capa oculta se utilizaron 300 neuronas todas con una funcion de activacion sigmoidal y la capa de salida dado que son 10 clases(digitos) esta constituida por 10 neuronas, con las mismas funciones de activacion de la capa de oculta.\n",
      "\n",
      "<img src=\"ANN.png\" width=\"500px\">\n",
      "\n",
      "#Algoritmo de Backpropagation \n",
      "\n",
      "El algoritmo de BackPropagation,  o propagacion hacia atras de los errores, es un algoritmo utilizado comunmente en el entrenamiento de redes neuronales. este utiliza como mecanismo de optimizacion el algoritmo de gradiente descendiente. \n",
      "\n",
      "Este metodo calcula el gradiente de la funcion de costo de la red nueroal con respecto a los parametros de la red neuronal comunmente conosidos como pesos. este calculo se utiliza para propagar el error atravez de toda la red y se actualizan los pesos correspondientes.\n",
      "\n",
      "A continuacion se describiran los elementos que componen el metodo.\n",
      "##Funcion de costo\n",
      "* $J(W,b; x,y) = \\frac{1}{2} \\left\\| h_{W,b}(x) - y \\right\\|^2$\n",
      "\n",
      "##Funcion de Activacion para cada neurona\n",
      "\n",
      "* $f(z) = \\frac{1}{1+\\exp(-z)}$\n",
      "\n",
      "## Modelo de un neurona\n",
      "Modelo matematico de una neurona. \n",
      "\n",
      "* $h_{W,b}(x) = f(W^Tx) = f(\\sum_{i=1}^3 W_{i}x_i +b)$\n",
      "\n",
      "Representacion grafica de una neurona.\n",
      "<img src=\"http://deeplearning.stanford.edu/wiki/images/3/3d/SingleNeuron.png\" width=\"300px\">\n",
      "\n",
      "\n",
      "#Implementacion del Metodo BackPropagation Forma no matricial\n",
      "\n",
      "A continuacion presentamos el pseudo codigo utilizado para realizar la implementaicon del metodo.\n",
      "\n",
      "1. Realizar la Propagacion hacia delante atravez de todas las capas de la red neuronal.\n",
      "\n",
      "2. Para cada neurona de la capa de salida calculamos el termino $\\delta^{(n_l)}_i\n",
      "= \\frac{\\partial}{\\partial z^{(n_l)}_i} \\;\\;\n",
      "        \\frac{1}{2} \\left\\|y - h_{W,b}(x)\\right\\|^2 = - (y_i - a^{(n_l)}_i) \\cdot f'(z^{(n_l)}_i)\n",
      "$ este termino es producto de realizar el proceso de optimizacion con respecto a los pesos contiguos a la capa de salida.\n",
      "\n",
      "3. para cada una de las capas ocultas realizamos el calculo del termino  $\\delta^{(l)}_i = \\left( \\sum_{j=1}^{s_{l+1}} W^{(l)}_{ji} \\delta^{(l+1)}_j \\right) f'(z^{(l)}_i)$. notemos que este termino depende del $\\delta^{(l+1)}_j$ dado que estamos realizando la optimizacion de los pesos de las capas ocultas hasta la capa de entrada y estos tiene cada vez mayor infuencia en las neuronas subsecuentes.\n",
      "\n",
      "4. Cuado hemos calculado los terminos $\\delta$ para la capa de salida y las capas ocultas ya podemos obtener el valor de las derivadas con respecto de cad uno de los pesos. \n",
      "\n",
      "    $\\begin{align*}\\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) &= a^{(l)}_j \\delta_i^{(l+1)} \\\\ \\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) &= \\delta_i^{(l+1)}.\\end{align*}$\n",
      "\n",
      "5. Finalmente realizamos la actualizacion de los pesos utilizando el algoritmo de gradiente descendiente.\n",
      "    \n",
      "    $\\begin{align}W_{ij}^{(l)} &= W_{ij}^{(l)} - \\alpha \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b) \\\\ b_{i}^{(l)} &= b_{i}^{(l)} - \\alpha \\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b) \\end{align}$\n",
      "    \n",
      "\n",
      "# Implementacion del metodo de forma matricial \n",
      "\n",
      "A continuacion presentamos la implementacion de forma matricial del algoritmo utilizando Codigo en C++.\n",
      "``` java\n",
      "       for (int iterations = 0; iterations < NUM_OF_ITERATIOS; ++iterations) {\n",
      "\t\t  // forward propagation\n",
      "\t\t  Z1 = W1.t()*X;\n",
      "\t\t  A1(span(1,A1.n_rows-1),span::all)=F(Z1);\n",
      "\t\t  Z2 = W2.t()*A1;\n",
      "\t\t  H\t = F(Z2);\n",
      "\n",
      "\t\t  // back propagation\n",
      "\t\t  D3  = -(Y-H.t()).t()%FP(H);\n",
      "\t\t  D2  = (W2*D3)%FP(A1);\n",
      "\t\t  DW2 = A1*D3.t();\n",
      "\t\t  DW1 = X*D2(span(1,A1.n_rows-1),span::all).t();\n",
      "\n",
      "\t\t  //update weight\n",
      "\t\t  W1=W1-(ALPHA/m)*(DW1);\n",
      "\t\t  W2=W2-(ALPHA/m)*(DW2);\n",
      "\t\t  J=join_cols(J,sum(square(H.t()-Y))/m);\n",
      "\t\t  cout<<iterations<<\":\"<<J(iterations,0)<<endl;\n",
      "\t  }    \n",
      "```\n",
      "\n",
      "Vemos que gracias a la implementacio del metodo de forma matricial no solo se simplifica la implemetacion del mismo si no que se puede generalizar para redes neuronales de $n$ caracteristicas y $l$ nerunas en la capa oculta y  $k$ neuronas en la capa de salida.\n",
      "\n",
      "Adicionalmente esta implementacion nos permite concentarnos en la implementacion y depuracion del metodo de forma independiente a el proceso de aceleracion, en este momento para realizar el proceso de acceleracion del metodo la tarea es concentrarse en que metodo de multiplicion de matrices utilizar.\n",
      "\n",
      "Esta implementacion utilizo la libreria [armadillo](http://arma.sourceforge.net/speed.html \"armadillo\") de algebra lineal que cuenta con la implementacion de las operaciones matriciales. \n",
      "\n",
      "\n",
      "# Pruebas Red neuronal\n",
      "\n",
      "Despues de entrenar la red neronal se implemento una aplicacion web para moviles para probar la capacidad de la red neuronal de clasificar de forma correcta los digitos escritos a mano para probar dicha aplicacion se necesita acceder a esta [URL](http://judge.utp.edu.co/ANN/ \"ANN\"). Utilizando un telefono movil o tablet.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}